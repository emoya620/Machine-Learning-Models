{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 1) Define the necessary imports for my code"]},{"cell_type":"code","execution_count":11,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-10T11:21:01.931565Z","iopub.status.busy":"2023-12-10T11:21:01.930677Z","iopub.status.idle":"2023-12-10T11:21:01.936937Z","shell.execute_reply":"2023-12-10T11:21:01.935929Z","shell.execute_reply.started":"2023-12-10T11:21:01.931531Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.utils.data as data\n","\n","import torchvision.transforms as transforms\n","\n","import numpy as np\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{},"source":["# 2) Copy the NumpyImageDataset.py code into this notebook"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T11:21:01.942652Z","iopub.status.busy":"2023-12-10T11:21:01.942183Z","iopub.status.idle":"2023-12-10T11:21:01.951112Z","shell.execute_reply":"2023-12-10T11:21:01.950144Z","shell.execute_reply.started":"2023-12-10T11:21:01.942628Z"},"trusted":true},"outputs":[],"source":["from __future__ import print_function\n","from PIL import Image\n","\n","class NumpyImageDataset(data.Dataset):\n","\n","    # image dataset from numpy arrays\n","\n","    # Example:\n","\n","    # num_classes = 20\n","    # train_data = np.load('../data/cifar_train_data.npy').transpose((0,2,3,1))\n","    # train_label = np.load('../data/cifar_train_label.npy')\n","    # trainset = NumpyImageDataset(train_data, train_label, transform=transform_train)\n","    # trainloader = data.DataLoader(trainset, batch_size=args.train_batch, shuffle=True, num_workers=args.workers)    \n","\n","    def __init__(self, data_array, label_array, transform=None, label_transform=None):\n","        self.data_tensor = data_array\n","        self.label_tensor = torch.from_numpy(label_array).type('torch.LongTensor')\n","        self.transform = transform\n","        self.label_transform = label_transform\n","        assert self.data_tensor.shape[0] == self.label_tensor.size(0)\n","\n","    def __getitem__(self, index):\n","\n","        img, label = self.data_tensor[index], self.label_tensor[index]\n","\n","        # change to PIL image\n","        img = Image.fromarray(img)\n","\n","        if self.transform is not None:\n","            # apply user-defined sequence of transformations\n","            img = self.transform(img)\n","\n","        if self.label_transform is not None:\n","            label = self.label_transform(label)\n","\n","        return img, label\n","\n","    def __len__(self):\n","        return self.data_tensor.shape[0]"]},{"cell_type":"markdown","metadata":{},"source":["# 3) Copy the generate_submission.py code into this notebook"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T11:21:01.952858Z","iopub.status.busy":"2023-12-10T11:21:01.952597Z","iopub.status.idle":"2023-12-10T11:21:01.962338Z","shell.execute_reply":"2023-12-10T11:21:01.961539Z","shell.execute_reply.started":"2023-12-10T11:21:01.952835Z"},"trusted":true},"outputs":[],"source":["import sys\n","\n","# You can also import his python and call the following function\n","# to generate the submission file\n","def writeSubmissionFile(labels, fn):\n","    f = open(fn, 'w')\n","    f.write(\"id,category\\n\")\n","    for ii, ll in enumerate(labels):\n","        f.write(str(ii) + \",\" + str(int(ll)) + \"\\n\")\n","\n","    f.close()"]},{"cell_type":"markdown","metadata":{},"source":["# 4) Define the transforms, and create the 'train_loader', 'validation_loader', and 'test_loader' variables for later"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T11:21:01.964271Z","iopub.status.busy":"2023-12-10T11:21:01.963958Z","iopub.status.idle":"2023-12-10T11:21:02.066809Z","shell.execute_reply":"2023-12-10T11:21:02.065960Z","shell.execute_reply.started":"2023-12-10T11:21:01.964247Z"},"trusted":true},"outputs":[],"source":["# Define the transforms for our training dataset\n","transform_train = transforms.Compose([transforms.ToTensor(),\n","                                      transforms.Normalize(mean = 0.5, std = 1),\n","                                      transforms.RandomGrayscale(p=0.2),\n","                                      transforms.RandomHorizontalFlip(p=0.2),\n","                                      transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)],p=0.2),\n","                                      transforms.RandomCrop(32, padding=4),\n","                                      transforms.RandomRotation(10)\n","                                     ])\n","\n","# Define the transforms for our validation and test datasets\n","transform_test = transforms.Compose([transforms.ToTensor(),\n","                                     transforms.Normalize(mean = 0.5, std = 1)])\n","\n","# Determine the size of the training set\n","train_size = 45000\n","\n","# Import the training data and label and split it into a train set and validation set\n","initial_data = np.load('/kaggle/input/model-data/cifar_train_data.npy').transpose((0,2,3,1))\n","initial_target = np.load('/kaggle/input/model-data/cifar_train_label.npy')\n","\n","train_data, train_target = initial_data[:train_size], initial_target[:train_size]\n","validation_data, validation_target = initial_data[train_size:], initial_target[train_size:]\n","\n","train_set = NumpyImageDataset(train_data, train_target, transform = transform_train)\n","train_loader = data.DataLoader(train_set, batch_size = 32, shuffle = True)\n","\n","validation_set = NumpyImageDataset(validation_data, validation_target, transform = transform_test)\n","validation_loader = data.DataLoader(validation_set, batch_size = 32)\n","\n","# Import the testing data\n","test_data = np.load('/kaggle/input/model-data/cifar_test_data.npy').transpose((0,2,3,1))\n","test_target = np.zeros(10000)\n","test_set = NumpyImageDataset(test_data, test_target, transform = transform_test)\n","test_loader = data.DataLoader(test_set, batch_size = 1)"]},{"cell_type":"markdown","metadata":{},"source":["# 5) Define some useful functions for training our model"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T11:21:02.068137Z","iopub.status.busy":"2023-12-10T11:21:02.067859Z","iopub.status.idle":"2023-12-10T11:21:02.074729Z","shell.execute_reply":"2023-12-10T11:21:02.073761Z","shell.execute_reply.started":"2023-12-10T11:21:02.068114Z"},"trusted":true},"outputs":[],"source":["# Carry out one training step of the model\n","def train_step(model, batch):\n","    images, targets = batch \n","    outputs = model(images)                  \n","    loss = F.cross_entropy(outputs, targets) \n","    return loss\n","\n","# Carry out one set of predictions on the validation set\n","def validation_step(model, batch):\n","    images, targets = batch\n","    outputs = model(images)\n","    acc = accuracy(outputs, targets)\n","    return acc\n","\n","# Determine the accuracy of predictions\n","def accuracy(outputs, targets):\n","    predictions = torch.max(outputs, dim=1)[1]\n","    return torch.tensor(torch.sum(predictions == targets).item() / len(predictions))"]},{"cell_type":"markdown","metadata":{},"source":["<h1> 6) Define the architecture of my Residual Network </h1>\n","<h3> Details: 10 Layers: 9 convolutional and 1 linear; 2 residual blocks </h3>"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T11:21:02.087408Z","iopub.status.busy":"2023-12-10T11:21:02.086909Z","iopub.status.idle":"2023-12-10T11:21:02.101605Z","shell.execute_reply":"2023-12-10T11:21:02.100644Z","shell.execute_reply.started":"2023-12-10T11:21:02.087382Z"},"trusted":true},"outputs":[],"source":["class ResNet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \n","        # Layer 1\n","        self.conv1 = nn.Sequential(nn.Conv2d(3, 32, kernel_size = 3, padding = 1), \n","                                   nn.BatchNorm2d(32), \n","                                   nn.ReLU(inplace = True))\n","        \n","        # Layer 2\n","        self.conv2 = nn.Sequential(nn.Conv2d(32, 64, kernel_size = 3, padding = 1), \n","                                   nn.BatchNorm2d(64), \n","                                   nn.ReLU(inplace = True))\n","        \n","        # Layer 3\n","        self.conv3 = nn.Sequential(nn.Conv2d(64, 128, kernel_size = 3, padding = 1), \n","                                   nn.BatchNorm2d(128), \n","                                   nn.ReLU(inplace = True),\n","                                   nn.MaxPool2d(2))\n","        \n","        # Layer 4 and 5\n","        self.resBlock1 = nn.Sequential(nn.Conv2d(128, 128, kernel_size = 3, padding = 1), \n","                                       nn.BatchNorm2d(128), \n","                                       nn.ReLU(inplace = True), \n","                                       nn.Conv2d(128, 128, kernel_size = 3, padding = 1), \n","                                       nn.BatchNorm2d(128), \n","                                       nn.ReLU(inplace = True))\n","        \n","        # Layer 6\n","        self.conv4 = nn.Sequential(nn.Conv2d(128, 256, kernel_size = 3, padding = 1), \n","                                   nn.BatchNorm2d(256), \n","                                   nn.ReLU(inplace = True),\n","                                   nn.MaxPool2d(2))\n","        \n","        # Layer 7\n","        self.conv5 = nn.Sequential(nn.Conv2d(256, 512, kernel_size = 3, padding = 1), \n","                                   nn.BatchNorm2d(512), \n","                                   nn.ReLU(inplace = True),\n","                                   nn.MaxPool2d(2))\n","        \n","        # Layer 8 and 9\n","        self.resBlock2 = nn.Sequential(nn.Conv2d(512, 512, kernel_size = 3, padding = 1), \n","                                       nn.BatchNorm2d(512), \n","                                       nn.ReLU(inplace = True), \n","                                       nn.Conv2d(512, 512, kernel_size = 3, padding = 1), \n","                                       nn.BatchNorm2d(512), \n","                                       nn.ReLU(inplace = True))\n","        \n","        # Layer 10\n","        self.linear1 = nn.Sequential(nn.MaxPool2d(4), \n","                                     nn.Flatten(), \n","                                     nn.Linear(512, 20))\n","        \n","    def forward(self, xb):\n","        out = self.conv1(xb)\n","        out = self.conv2(out)\n","        out = self.conv3(out)\n","        out = self.resBlock1(out) + out\n","        out = self.conv4(out)\n","        out = self.conv5(out)\n","        out = self.resBlock2(out) + out\n","        out = self.linear1(out)\n","        return out"]},{"cell_type":"markdown","metadata":{},"source":["# 7) Define the necessary methods to load data so that it can be processed with a GPU"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T11:21:02.103675Z","iopub.status.busy":"2023-12-10T11:21:02.103369Z","iopub.status.idle":"2023-12-10T11:21:02.116763Z","shell.execute_reply":"2023-12-10T11:21:02.116027Z","shell.execute_reply.started":"2023-12-10T11:21:02.103652Z"},"trusted":true},"outputs":[],"source":["# Define the to_device method\n","def to_device(data, device):\n","    \"\"\"Move tensor(s) to chosen device\"\"\"\n","    if isinstance(data, (list,tuple)):\n","        return [to_device(x, device) for x in data]\n","    return data.to(device, non_blocking=True)\n","\n","# Define the GPUDataLoader class\n","class GPUDataLoader():\n","    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n","    def __init__(self, dl, device):\n","        self.dl = dl\n","        self.device = device\n","        \n","    def __iter__(self):\n","        \"\"\"Yield a batch of data after moving it to device\"\"\"\n","        for b in self.dl: \n","            yield to_device(b, self.device)\n","\n","    def __len__(self):\n","        \"\"\"Number of batches\"\"\"\n","        return len(self.dl)\n","\n","# Define the GPU we want to use\n","gpu = torch.device('cuda')\n","\n","# Load the data with the GPUDataLoader class\n","train_loader = GPUDataLoader(train_loader, gpu)\n","validation_loader = GPUDataLoader(validation_loader, gpu)\n","test_loader = GPUDataLoader(test_loader, gpu)"]},{"cell_type":"markdown","metadata":{},"source":["# 8) Define the 'fit' method for training our dataset"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T11:21:02.118519Z","iopub.status.busy":"2023-12-10T11:21:02.117910Z","iopub.status.idle":"2023-12-10T11:21:02.130884Z","shell.execute_reply":"2023-12-10T11:21:02.130148Z","shell.execute_reply.started":"2023-12-10T11:21:02.118487Z"},"trusted":true},"outputs":[],"source":["# Define the 'fit' function which trains our model on the training and validation set\n","def fit(epochs, lr, model, train_loader, validation_loader, opt_func = torch.optim.Adam):\n","    optimizer = opt_func(model.parameters(), lr)\n","    \n","    # Train the model for the given number of epochs\n","    for epoch in range(epochs):\n","        \n","        # Training the model \n","        model.train()\n","        train_losses = []\n","        for batch in train_loader:\n","            loss = train_step(model, batch)\n","            train_losses.append(loss)\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","        \n","        # Test the model on the validation dataset\n","        model.eval()\n","        outputs = [validation_step(model, batch) for batch in validation_loader]\n","        result = np.mean(outputs)\n","        print(\"Epoch:\", epoch + 1, \"Validation Accuracy: {:.2f}\".format(100 * result))\n","\n","    print(\"The model has been trained\")"]},{"cell_type":"markdown","metadata":{},"source":["# 9) Train our model and define the hyperparameters"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T11:21:02.132388Z","iopub.status.busy":"2023-12-10T11:21:02.132063Z","iopub.status.idle":"2023-12-10T12:09:02.137672Z","shell.execute_reply":"2023-12-10T12:09:02.136659Z","shell.execute_reply.started":"2023-12-10T11:21:02.132356Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch Size: 32 Learning Rate: 0.0001\n","Epoch: 1 Validation Accuracy: 47.61\n","Epoch: 2 Validation Accuracy: 57.42\n","Epoch: 3 Validation Accuracy: 61.05\n","Epoch: 4 Validation Accuracy: 65.09\n","Epoch: 5 Validation Accuracy: 68.19\n","Epoch: 6 Validation Accuracy: 69.51\n","Epoch: 7 Validation Accuracy: 70.26\n","Epoch: 8 Validation Accuracy: 72.03\n","Epoch: 9 Validation Accuracy: 72.89\n","Epoch: 10 Validation Accuracy: 74.88\n","Epoch: 11 Validation Accuracy: 74.26\n","Epoch: 12 Validation Accuracy: 76.61\n","Epoch: 13 Validation Accuracy: 77.69\n","Epoch: 14 Validation Accuracy: 76.71\n","Epoch: 15 Validation Accuracy: 77.67\n","Epoch: 16 Validation Accuracy: 77.65\n","Epoch: 17 Validation Accuracy: 78.74\n","Epoch: 18 Validation Accuracy: 79.48\n","Epoch: 19 Validation Accuracy: 79.58\n","Epoch: 20 Validation Accuracy: 79.20\n","Epoch: 21 Validation Accuracy: 78.78\n","Epoch: 22 Validation Accuracy: 79.86\n","Epoch: 23 Validation Accuracy: 80.45\n","Epoch: 24 Validation Accuracy: 81.43\n","Epoch: 25 Validation Accuracy: 80.33\n","Epoch: 26 Validation Accuracy: 81.85\n","Epoch: 27 Validation Accuracy: 81.29\n","Epoch: 28 Validation Accuracy: 81.57\n","Epoch: 29 Validation Accuracy: 81.47\n","Epoch: 30 Validation Accuracy: 81.35\n","Epoch: 31 Validation Accuracy: 82.42\n","Epoch: 32 Validation Accuracy: 82.42\n","Epoch: 33 Validation Accuracy: 81.99\n","Epoch: 34 Validation Accuracy: 82.30\n","Epoch: 35 Validation Accuracy: 82.48\n","Epoch: 36 Validation Accuracy: 81.65\n","Epoch: 37 Validation Accuracy: 82.78\n","Epoch: 38 Validation Accuracy: 82.38\n","Epoch: 39 Validation Accuracy: 82.38\n","Epoch: 40 Validation Accuracy: 82.48\n","Epoch: 41 Validation Accuracy: 82.19\n","Epoch: 42 Validation Accuracy: 82.42\n","Epoch: 43 Validation Accuracy: 83.02\n","Epoch: 44 Validation Accuracy: 83.06\n","Epoch: 45 Validation Accuracy: 82.90\n","Epoch: 46 Validation Accuracy: 82.29\n","Epoch: 47 Validation Accuracy: 83.66\n","Epoch: 48 Validation Accuracy: 82.82\n","Epoch: 49 Validation Accuracy: 82.56\n","Epoch: 50 Validation Accuracy: 83.24\n","The model has been trained\n"]}],"source":["# Instantiate the model\n","model = to_device(ResNet(), gpu)\n","\n","# Define some hyperparameters\n","num_epochs = 50\n","opt_func = torch.optim.Adam\n","lr = 0.0001\n","\n","# Train the model\n","print(\"Batch Size:\", 32, \"Learning Rate:\", lr)\n","fit(num_epochs, lr, model, train_loader, validation_loader, opt_func)"]},{"cell_type":"markdown","metadata":{},"source":["# 10) Use our trained model to make predictions on the test set, and write the predictions to a .csv file"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-12-10T12:09:02.140309Z","iopub.status.busy":"2023-12-10T12:09:02.139986Z","iopub.status.idle":"2023-12-10T12:09:24.319489Z","shell.execute_reply":"2023-12-10T12:09:24.318484Z","shell.execute_reply.started":"2023-12-10T12:09:02.140283Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[ 7 11 16 ...  0  2  8]\n","[[   0    7]\n"," [   1   11]\n"," [   2   16]\n"," ...\n"," [9997    0]\n"," [9998    2]\n"," [9999    8]]\n"]}],"source":["ids = np.arange(0, len(test_set))\n","predictions = np.array([0])\n","\n","for data, targets in test_loader:\n","    output = model(data)\n","    preds = torch.max(output, dim=1)[1]\n","    preds = preds.detach().cpu().numpy()\n","    predictions = np.concatenate((predictions, preds))\n","\n","predictions = np.delete(predictions, 0)\n","print(predictions)\n","\n","results = np.column_stack((ids, predictions))\n","print(results)\n","\n","writeSubmissionFile(predictions, 'emilio_moya_submissions.csv')"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4052544,"sourceId":7043208,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
